{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from statistics import mean\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "tf.random.set_random_seed(47)\n",
    "\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature_0</th>\n",
       "      <th>feature_1</th>\n",
       "      <th>feature_2</th>\n",
       "      <th>feature_3</th>\n",
       "      <th>feature_4</th>\n",
       "      <th>feature_5</th>\n",
       "      <th>feature_6</th>\n",
       "      <th>feature_7</th>\n",
       "      <th>feature_8</th>\n",
       "      <th>feature_9</th>\n",
       "      <th>...</th>\n",
       "      <th>target_4</th>\n",
       "      <th>target_5</th>\n",
       "      <th>target_6</th>\n",
       "      <th>target_7</th>\n",
       "      <th>target_8</th>\n",
       "      <th>target_9</th>\n",
       "      <th>target_10</th>\n",
       "      <th>target_11</th>\n",
       "      <th>target_12</th>\n",
       "      <th>target_13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>count</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "      <td>2417.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>mean</td>\n",
       "      <td>0.001173</td>\n",
       "      <td>-0.000436</td>\n",
       "      <td>-0.000257</td>\n",
       "      <td>0.000265</td>\n",
       "      <td>0.001228</td>\n",
       "      <td>0.000475</td>\n",
       "      <td>0.001107</td>\n",
       "      <td>0.000420</td>\n",
       "      <td>0.001076</td>\n",
       "      <td>-0.000009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.298717</td>\n",
       "      <td>0.247000</td>\n",
       "      <td>0.177079</td>\n",
       "      <td>0.198593</td>\n",
       "      <td>0.073645</td>\n",
       "      <td>0.104675</td>\n",
       "      <td>0.119570</td>\n",
       "      <td>0.751345</td>\n",
       "      <td>0.744311</td>\n",
       "      <td>0.014067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>std</td>\n",
       "      <td>0.097411</td>\n",
       "      <td>0.097885</td>\n",
       "      <td>0.097746</td>\n",
       "      <td>0.096969</td>\n",
       "      <td>0.096909</td>\n",
       "      <td>0.097306</td>\n",
       "      <td>0.097170</td>\n",
       "      <td>0.096803</td>\n",
       "      <td>0.096326</td>\n",
       "      <td>0.096805</td>\n",
       "      <td>...</td>\n",
       "      <td>0.457790</td>\n",
       "      <td>0.431356</td>\n",
       "      <td>0.381815</td>\n",
       "      <td>0.399024</td>\n",
       "      <td>0.261246</td>\n",
       "      <td>0.306198</td>\n",
       "      <td>0.324525</td>\n",
       "      <td>0.432323</td>\n",
       "      <td>0.436338</td>\n",
       "      <td>0.117792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>min</td>\n",
       "      <td>-0.371146</td>\n",
       "      <td>-0.472632</td>\n",
       "      <td>-0.339195</td>\n",
       "      <td>-0.467945</td>\n",
       "      <td>-0.367044</td>\n",
       "      <td>-0.509447</td>\n",
       "      <td>-0.319928</td>\n",
       "      <td>-0.594498</td>\n",
       "      <td>-0.369712</td>\n",
       "      <td>-0.767128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25%</td>\n",
       "      <td>-0.053655</td>\n",
       "      <td>-0.058734</td>\n",
       "      <td>-0.057526</td>\n",
       "      <td>-0.057149</td>\n",
       "      <td>-0.058461</td>\n",
       "      <td>-0.060212</td>\n",
       "      <td>-0.058445</td>\n",
       "      <td>-0.062849</td>\n",
       "      <td>-0.063472</td>\n",
       "      <td>-0.065010</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50%</td>\n",
       "      <td>0.003649</td>\n",
       "      <td>-0.003513</td>\n",
       "      <td>0.002892</td>\n",
       "      <td>-0.000153</td>\n",
       "      <td>0.005565</td>\n",
       "      <td>0.000321</td>\n",
       "      <td>0.006179</td>\n",
       "      <td>0.001436</td>\n",
       "      <td>0.003515</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75%</td>\n",
       "      <td>0.057299</td>\n",
       "      <td>0.048047</td>\n",
       "      <td>0.061007</td>\n",
       "      <td>0.054522</td>\n",
       "      <td>0.066286</td>\n",
       "      <td>0.059908</td>\n",
       "      <td>0.068892</td>\n",
       "      <td>0.061418</td>\n",
       "      <td>0.064958</td>\n",
       "      <td>0.063096</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>max</td>\n",
       "      <td>0.520272</td>\n",
       "      <td>0.614114</td>\n",
       "      <td>0.353241</td>\n",
       "      <td>0.568960</td>\n",
       "      <td>0.307649</td>\n",
       "      <td>0.336971</td>\n",
       "      <td>0.351401</td>\n",
       "      <td>0.454591</td>\n",
       "      <td>0.419852</td>\n",
       "      <td>0.420876</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 117 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         feature_0    feature_1    feature_2    feature_3    feature_4  \\\n",
       "count  2417.000000  2417.000000  2417.000000  2417.000000  2417.000000   \n",
       "mean      0.001173    -0.000436    -0.000257     0.000265     0.001228   \n",
       "std       0.097411     0.097885     0.097746     0.096969     0.096909   \n",
       "min      -0.371146    -0.472632    -0.339195    -0.467945    -0.367044   \n",
       "25%      -0.053655    -0.058734    -0.057526    -0.057149    -0.058461   \n",
       "50%       0.003649    -0.003513     0.002892    -0.000153     0.005565   \n",
       "75%       0.057299     0.048047     0.061007     0.054522     0.066286   \n",
       "max       0.520272     0.614114     0.353241     0.568960     0.307649   \n",
       "\n",
       "         feature_5    feature_6    feature_7    feature_8    feature_9  ...  \\\n",
       "count  2417.000000  2417.000000  2417.000000  2417.000000  2417.000000  ...   \n",
       "mean      0.000475     0.001107     0.000420     0.001076    -0.000009  ...   \n",
       "std       0.097306     0.097170     0.096803     0.096326     0.096805  ...   \n",
       "min      -0.509447    -0.319928    -0.594498    -0.369712    -0.767128  ...   \n",
       "25%      -0.060212    -0.058445    -0.062849    -0.063472    -0.065010  ...   \n",
       "50%       0.000321     0.006179     0.001436     0.003515     0.002432  ...   \n",
       "75%       0.059908     0.068892     0.061418     0.064958     0.063096  ...   \n",
       "max       0.336971     0.351401     0.454591     0.419852     0.420876  ...   \n",
       "\n",
       "          target_4     target_5     target_6     target_7     target_8  \\\n",
       "count  2417.000000  2417.000000  2417.000000  2417.000000  2417.000000   \n",
       "mean      0.298717     0.247000     0.177079     0.198593     0.073645   \n",
       "std       0.457790     0.431356     0.381815     0.399024     0.261246   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "75%       1.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000   \n",
       "\n",
       "          target_9    target_10    target_11    target_12    target_13  \n",
       "count  2417.000000  2417.000000  2417.000000  2417.000000  2417.000000  \n",
       "mean      0.104675     0.119570     0.751345     0.744311     0.014067  \n",
       "std       0.306198     0.324525     0.432323     0.436338     0.117792  \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000  \n",
       "25%       0.000000     0.000000     1.000000     0.000000     0.000000  \n",
       "50%       0.000000     0.000000     1.000000     1.000000     0.000000  \n",
       "75%       0.000000     0.000000     1.000000     1.000000     0.000000  \n",
       "max       1.000000     1.000000     1.000000     1.000000     1.000000  \n",
       "\n",
       "[8 rows x 117 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"data/multilabel_dataset.csv\")\n",
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:, :-14].values\n",
    "Y = df.iloc[:, -14:].values\n",
    "cv = KFold(n_splits=5, random_state=37)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scikit-learn baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(solver=\"lbfgs\")\n",
    "clf = OneVsRestClassifier(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged f1 on cross validation: 0.6342384766906916\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for train, test in cv.split(X, Y):\n",
    "    clf.fit(X[train], Y[train])\n",
    "    Y_pred = clf.predict(X[test])\n",
    "    score = f1_score(Y[test], Y_pred, average=\"micro\")\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Micro-averaged f1 on cross validation: {mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pure tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "learning_rate = 0.03\n",
    "num_epochs = 100\n",
    "\n",
    "# Dimensions\n",
    "num_features = len(X[0])\n",
    "num_labels = len(Y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders for features and labels\n",
    "X_tensor = tf.placeholder(tf.float32, name=\"features\")\n",
    "Y_tensor = tf.placeholder(tf.float32, name=\"labels\")\n",
    "\n",
    "# Create variables for weights and bias\n",
    "w = tf.get_variable(\n",
    "    shape=(num_features, num_labels),\n",
    "    initializer=tf.random_normal_initializer(),\n",
    "    name=\"weights\",\n",
    ")\n",
    "b = tf.get_variable(\n",
    "    shape=(1, num_labels), initializer=tf.zeros_initializer(), name=\"bias\"\n",
    ")\n",
    "\n",
    "# Build a model returning logits\n",
    "logits = tf.matmul(X_tensor, w) + b\n",
    "\n",
    "# Define loss function. Unlike the single-label case, we should not output\n",
    "# a softmax probability distribultion as labels are classified independently.\n",
    "# Instead we apply a sigmoid on the logits as they are independent logistic regressions.\n",
    "# Since we treat each logit as an independent logistic regression, we need to sum\n",
    "# so that the whole model's performance is the sum of its per-class performances\n",
    "loss = tf.reduce_mean(\n",
    "    tf.reduce_sum(\n",
    "        tf.nn.sigmoid_cross_entropy_with_logits(logits=logits, labels=Y_tensor), axis=1\n",
    "    )\n",
    ")\n",
    "\n",
    "# Define training operation\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "\n",
    "# Make prediction\n",
    "def multi_label_hot(prediction, threshold=0.5):\n",
    "    prediction = tf.cast(prediction, tf.float32)\n",
    "    return tf.cast(tf.greater(prediction, threshold), tf.int64)\n",
    "\n",
    "\n",
    "prediction = tf.sigmoid(logits)\n",
    "one_hot_prediction = multi_label_hot(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged f1 on cross validation: 0.6306492826435719\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for train, test in cv.split(X, Y):\n",
    "    with tf.Session() as sess:\n",
    "        # Initialize variables\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        # Train model\n",
    "        for epoch in range(num_epochs):\n",
    "            _, l = sess.run(\n",
    "                [optimizer, loss], feed_dict={X_tensor: X[train], Y_tensor: Y[train]}\n",
    "            )\n",
    "\n",
    "        # Calculate predicted values\n",
    "        Y_pred = sess.run(one_hot_prediction, {X_tensor: X[test], Y_tensor: Y[test]})\n",
    "\n",
    "    score = f1_score(Y[test], Y_pred, average=\"micro\")\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Micro-averaged f1 on cross validation: {mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Micro-averaged f1 on cross validation: 0.6309615543717694\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for train, test in cv.split(X, Y):\n",
    "    # Create and compile model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(num_labels, activation=\"sigmoid\", input_shape=(num_features,)))\n",
    "\n",
    "    adam = optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam, loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Fit and make prediction\n",
    "    model.fit(X[train], Y[train], epochs=num_epochs, batch_size=200, verbose=0)\n",
    "    Y_pred = (model.predict(X[test]) > 0.5).astype(np.uint8)\n",
    "\n",
    "    score = f1_score(Y[test], Y_pred, average=\"micro\")\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Micro-averaged f1 on cross validation: {mean(scores)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keras with nonlinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1353 samples, validate on 580 samples\n",
      "Epoch 1/100\n",
      "1353/1353 [==============================] - 1s 932us/step - loss: 0.6734 - acc: 0.6457 - val_loss: 0.6394 - val_acc: 0.7724\n",
      "Epoch 2/100\n",
      "1353/1353 [==============================] - 0s 51us/step - loss: 0.5999 - acc: 0.7662 - val_loss: 0.5442 - val_acc: 0.7723\n",
      "Epoch 3/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.5236 - acc: 0.7683 - val_loss: 0.5064 - val_acc: 0.7723\n",
      "Epoch 4/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.5023 - acc: 0.7690 - val_loss: 0.4916 - val_acc: 0.7768\n",
      "Epoch 5/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4907 - acc: 0.7691 - val_loss: 0.4906 - val_acc: 0.7766\n",
      "Epoch 6/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4845 - acc: 0.7711 - val_loss: 0.4815 - val_acc: 0.7793\n",
      "Epoch 7/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4762 - acc: 0.7783 - val_loss: 0.4751 - val_acc: 0.7807\n",
      "Epoch 8/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4701 - acc: 0.7820 - val_loss: 0.4699 - val_acc: 0.7805\n",
      "Epoch 9/100\n",
      "1353/1353 [==============================] - 0s 51us/step - loss: 0.4613 - acc: 0.7853 - val_loss: 0.4646 - val_acc: 0.7847\n",
      "Epoch 10/100\n",
      "1353/1353 [==============================] - 0s 55us/step - loss: 0.4564 - acc: 0.7908 - val_loss: 0.4594 - val_acc: 0.7894\n",
      "Epoch 11/100\n",
      "1353/1353 [==============================] - 0s 56us/step - loss: 0.4469 - acc: 0.7952 - val_loss: 0.4550 - val_acc: 0.7913\n",
      "Epoch 12/100\n",
      "1353/1353 [==============================] - 0s 58us/step - loss: 0.4411 - acc: 0.7978 - val_loss: 0.4518 - val_acc: 0.7927\n",
      "Epoch 13/100\n",
      "1353/1353 [==============================] - 0s 57us/step - loss: 0.4387 - acc: 0.8012 - val_loss: 0.4491 - val_acc: 0.7937\n",
      "Epoch 14/100\n",
      "1353/1353 [==============================] - 0s 56us/step - loss: 0.4328 - acc: 0.8016 - val_loss: 0.4471 - val_acc: 0.7943\n",
      "Epoch 15/100\n",
      "1353/1353 [==============================] - 0s 53us/step - loss: 0.4286 - acc: 0.8056 - val_loss: 0.4456 - val_acc: 0.7940\n",
      "Epoch 16/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4260 - acc: 0.8049 - val_loss: 0.4450 - val_acc: 0.7950\n",
      "Epoch 17/100\n",
      "1353/1353 [==============================] - 0s 45us/step - loss: 0.4203 - acc: 0.8079 - val_loss: 0.4432 - val_acc: 0.7964\n",
      "Epoch 18/100\n",
      "1353/1353 [==============================] - 0s 41us/step - loss: 0.4150 - acc: 0.8122 - val_loss: 0.4428 - val_acc: 0.7958\n",
      "Epoch 19/100\n",
      "1353/1353 [==============================] - 0s 42us/step - loss: 0.4127 - acc: 0.8122 - val_loss: 0.4423 - val_acc: 0.7983\n",
      "Epoch 20/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.4077 - acc: 0.8146 - val_loss: 0.4420 - val_acc: 0.7991\n",
      "Epoch 21/100\n",
      "1353/1353 [==============================] - 0s 51us/step - loss: 0.4022 - acc: 0.8167 - val_loss: 0.4414 - val_acc: 0.7972\n",
      "Epoch 22/100\n",
      "1353/1353 [==============================] - 0s 54us/step - loss: 0.3988 - acc: 0.8185 - val_loss: 0.4412 - val_acc: 0.8020\n",
      "Epoch 23/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.3945 - acc: 0.8205 - val_loss: 0.4410 - val_acc: 0.8037\n",
      "Epoch 24/100\n",
      "1353/1353 [==============================] - 0s 39us/step - loss: 0.3894 - acc: 0.8234 - val_loss: 0.4422 - val_acc: 0.8011\n",
      "Epoch 25/100\n",
      "1353/1353 [==============================] - 0s 40us/step - loss: 0.3847 - acc: 0.8243 - val_loss: 0.4428 - val_acc: 0.8000\n",
      "Epoch 26/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.3815 - acc: 0.8274 - val_loss: 0.4445 - val_acc: 0.8022\n",
      "Epoch 27/100\n",
      "1353/1353 [==============================] - 0s 38us/step - loss: 0.3776 - acc: 0.8279 - val_loss: 0.4448 - val_acc: 0.8006\n",
      "Epoch 28/100\n",
      "1353/1353 [==============================] - 0s 42us/step - loss: 0.3742 - acc: 0.8321 - val_loss: 0.4459 - val_acc: 0.8016\n",
      "Epoch 00028: early stopping\n",
      "Train on 1353 samples, validate on 580 samples\n",
      "Epoch 1/100\n",
      "1353/1353 [==============================] - 1s 995us/step - loss: 0.6643 - acc: 0.7032 - val_loss: 0.6272 - val_acc: 0.7649\n",
      "Epoch 2/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.5813 - acc: 0.7619 - val_loss: 0.5230 - val_acc: 0.7714\n",
      "Epoch 3/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.5183 - acc: 0.7657 - val_loss: 0.5036 - val_acc: 0.7723\n",
      "Epoch 4/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.5031 - acc: 0.7679 - val_loss: 0.4862 - val_acc: 0.7711\n",
      "Epoch 5/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4921 - acc: 0.7683 - val_loss: 0.4840 - val_acc: 0.7794\n",
      "Epoch 6/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4837 - acc: 0.7746 - val_loss: 0.4727 - val_acc: 0.7796\n",
      "Epoch 7/100\n",
      "1353/1353 [==============================] - 0s 53us/step - loss: 0.4735 - acc: 0.7780 - val_loss: 0.4672 - val_acc: 0.7828\n",
      "Epoch 8/100\n",
      "1353/1353 [==============================] - 0s 60us/step - loss: 0.4667 - acc: 0.7834 - val_loss: 0.4626 - val_acc: 0.7883\n",
      "Epoch 9/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.4592 - acc: 0.7881 - val_loss: 0.4573 - val_acc: 0.7921\n",
      "Epoch 10/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4505 - acc: 0.7932 - val_loss: 0.4522 - val_acc: 0.7940\n",
      "Epoch 11/100\n",
      "1353/1353 [==============================] - 0s 42us/step - loss: 0.4459 - acc: 0.7959 - val_loss: 0.4487 - val_acc: 0.7948\n",
      "Epoch 12/100\n",
      "1353/1353 [==============================] - 0s 43us/step - loss: 0.4429 - acc: 0.7983 - val_loss: 0.4464 - val_acc: 0.7958\n",
      "Epoch 13/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4369 - acc: 0.7999 - val_loss: 0.4438 - val_acc: 0.7975\n",
      "Epoch 14/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.4308 - acc: 0.8011 - val_loss: 0.4422 - val_acc: 0.7988\n",
      "Epoch 15/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.4269 - acc: 0.8031 - val_loss: 0.4417 - val_acc: 0.7990\n",
      "Epoch 16/100\n",
      "1353/1353 [==============================] - 0s 45us/step - loss: 0.4220 - acc: 0.8056 - val_loss: 0.4406 - val_acc: 0.8009\n",
      "Epoch 17/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.4163 - acc: 0.8085 - val_loss: 0.4407 - val_acc: 0.7998\n",
      "Epoch 18/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.4154 - acc: 0.8081 - val_loss: 0.4402 - val_acc: 0.7995\n",
      "Epoch 19/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.4104 - acc: 0.8099 - val_loss: 0.4405 - val_acc: 0.7984\n",
      "Epoch 20/100\n",
      "1353/1353 [==============================] - 0s 51us/step - loss: 0.4056 - acc: 0.8154 - val_loss: 0.4399 - val_acc: 0.7993\n",
      "Epoch 21/100\n",
      "1353/1353 [==============================] - 0s 53us/step - loss: 0.4031 - acc: 0.8167 - val_loss: 0.4396 - val_acc: 0.7979\n",
      "Epoch 22/100\n",
      "1353/1353 [==============================] - 0s 52us/step - loss: 0.3977 - acc: 0.8175 - val_loss: 0.4399 - val_acc: 0.7989\n",
      "Epoch 23/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.3936 - acc: 0.8200 - val_loss: 0.4408 - val_acc: 0.7970\n",
      "Epoch 24/100\n",
      "1353/1353 [==============================] - 0s 51us/step - loss: 0.3889 - acc: 0.8233 - val_loss: 0.4408 - val_acc: 0.8006\n",
      "Epoch 25/100\n",
      "1353/1353 [==============================] - 0s 54us/step - loss: 0.3841 - acc: 0.8258 - val_loss: 0.4418 - val_acc: 0.7985\n",
      "Epoch 26/100\n",
      "1353/1353 [==============================] - 0s 54us/step - loss: 0.3797 - acc: 0.8285 - val_loss: 0.4445 - val_acc: 0.7980\n",
      "Epoch 00026: early stopping\n",
      "Train on 1353 samples, validate on 581 samples\n",
      "Epoch 1/100\n",
      "1353/1353 [==============================] - 1s 1ms/step - loss: 0.6749 - acc: 0.6415 - val_loss: 0.6412 - val_acc: 0.7692\n",
      "Epoch 2/100\n",
      "1353/1353 [==============================] - 0s 52us/step - loss: 0.6018 - acc: 0.7519 - val_loss: 0.5381 - val_acc: 0.7726\n",
      "Epoch 3/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.5199 - acc: 0.7652 - val_loss: 0.5009 - val_acc: 0.7724\n",
      "Epoch 4/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.5079 - acc: 0.7669 - val_loss: 0.4903 - val_acc: 0.7770\n",
      "Epoch 5/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4948 - acc: 0.7683 - val_loss: 0.4865 - val_acc: 0.7810\n",
      "Epoch 6/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.4863 - acc: 0.7739 - val_loss: 0.4778 - val_acc: 0.7801\n",
      "Epoch 7/100\n",
      "1353/1353 [==============================] - 0s 51us/step - loss: 0.4784 - acc: 0.7755 - val_loss: 0.4704 - val_acc: 0.7836\n",
      "Epoch 8/100\n",
      "1353/1353 [==============================] - 0s 53us/step - loss: 0.4699 - acc: 0.7821 - val_loss: 0.4647 - val_acc: 0.7862\n",
      "Epoch 9/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4648 - acc: 0.7870 - val_loss: 0.4597 - val_acc: 0.7921\n",
      "Epoch 10/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4564 - acc: 0.7903 - val_loss: 0.4553 - val_acc: 0.7912\n",
      "Epoch 11/100\n",
      "1353/1353 [==============================] - 0s 51us/step - loss: 0.4499 - acc: 0.7931 - val_loss: 0.4515 - val_acc: 0.7944\n",
      "Epoch 12/100\n",
      "1353/1353 [==============================] - 0s 42us/step - loss: 0.4469 - acc: 0.7959 - val_loss: 0.4495 - val_acc: 0.7962\n",
      "Epoch 13/100\n",
      "1353/1353 [==============================] - 0s 45us/step - loss: 0.4435 - acc: 0.7963 - val_loss: 0.4475 - val_acc: 0.7980\n",
      "Epoch 14/100\n",
      "1353/1353 [==============================] - 0s 39us/step - loss: 0.4366 - acc: 0.8008 - val_loss: 0.4457 - val_acc: 0.7962\n",
      "Epoch 15/100\n",
      "1353/1353 [==============================] - 0s 43us/step - loss: 0.4315 - acc: 0.8025 - val_loss: 0.4449 - val_acc: 0.8008\n",
      "Epoch 16/100\n",
      "1353/1353 [==============================] - 0s 41us/step - loss: 0.4303 - acc: 0.8057 - val_loss: 0.4430 - val_acc: 0.8010\n",
      "Epoch 17/100\n",
      "1353/1353 [==============================] - 0s 39us/step - loss: 0.4252 - acc: 0.8065 - val_loss: 0.4420 - val_acc: 0.8012\n",
      "Epoch 18/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.4191 - acc: 0.8123 - val_loss: 0.4412 - val_acc: 0.8046\n",
      "Epoch 19/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.4155 - acc: 0.8107 - val_loss: 0.4410 - val_acc: 0.8034\n",
      "Epoch 20/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.4112 - acc: 0.8131 - val_loss: 0.4406 - val_acc: 0.8044\n",
      "Epoch 21/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4083 - acc: 0.8160 - val_loss: 0.4402 - val_acc: 0.8046\n",
      "Epoch 22/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.4060 - acc: 0.8180 - val_loss: 0.4399 - val_acc: 0.8043\n",
      "Epoch 23/100\n",
      "1353/1353 [==============================] - 0s 45us/step - loss: 0.4000 - acc: 0.8176 - val_loss: 0.4399 - val_acc: 0.8040\n",
      "Epoch 24/100\n",
      "1353/1353 [==============================] - 0s 45us/step - loss: 0.3973 - acc: 0.8208 - val_loss: 0.4405 - val_acc: 0.8043\n",
      "Epoch 25/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.3908 - acc: 0.8241 - val_loss: 0.4409 - val_acc: 0.8046\n",
      "Epoch 26/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.3860 - acc: 0.8249 - val_loss: 0.4413 - val_acc: 0.8039\n",
      "Epoch 27/100\n",
      "1353/1353 [==============================] - 0s 54us/step - loss: 0.3834 - acc: 0.8278 - val_loss: 0.4414 - val_acc: 0.8035\n",
      "Epoch 28/100\n",
      "1353/1353 [==============================] - 0s 52us/step - loss: 0.3781 - acc: 0.8291 - val_loss: 0.4428 - val_acc: 0.8040\n",
      "Epoch 00028: early stopping\n",
      "Train on 1353 samples, validate on 581 samples\n",
      "Epoch 1/100\n",
      "1353/1353 [==============================] - 2s 1ms/step - loss: 0.6776 - acc: 0.6324 - val_loss: 0.6481 - val_acc: 0.7472\n",
      "Epoch 2/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.6093 - acc: 0.7567 - val_loss: 0.5548 - val_acc: 0.7744\n",
      "Epoch 3/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.5303 - acc: 0.7650 - val_loss: 0.5105 - val_acc: 0.7727\n",
      "Epoch 4/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.5091 - acc: 0.7678 - val_loss: 0.4913 - val_acc: 0.7721\n",
      "Epoch 5/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4917 - acc: 0.7690 - val_loss: 0.4900 - val_acc: 0.7781\n",
      "Epoch 6/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4854 - acc: 0.7734 - val_loss: 0.4821 - val_acc: 0.7836\n",
      "Epoch 7/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.4773 - acc: 0.7761 - val_loss: 0.4756 - val_acc: 0.7820\n",
      "Epoch 8/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.4686 - acc: 0.7843 - val_loss: 0.4703 - val_acc: 0.7836\n",
      "Epoch 9/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4629 - acc: 0.7869 - val_loss: 0.4660 - val_acc: 0.7895\n",
      "Epoch 10/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.4566 - acc: 0.7918 - val_loss: 0.4604 - val_acc: 0.7924\n",
      "Epoch 11/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4501 - acc: 0.7945 - val_loss: 0.4556 - val_acc: 0.7952\n",
      "Epoch 12/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4449 - acc: 0.7962 - val_loss: 0.4518 - val_acc: 0.7963\n",
      "Epoch 13/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4404 - acc: 0.7987 - val_loss: 0.4488 - val_acc: 0.7973\n",
      "Epoch 14/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4347 - acc: 0.8018 - val_loss: 0.4463 - val_acc: 0.7970\n",
      "Epoch 15/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.4285 - acc: 0.8061 - val_loss: 0.4441 - val_acc: 0.7974\n",
      "Epoch 16/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4262 - acc: 0.8073 - val_loss: 0.4426 - val_acc: 0.7994\n",
      "Epoch 17/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.4231 - acc: 0.8069 - val_loss: 0.4407 - val_acc: 0.8002\n",
      "Epoch 18/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4180 - acc: 0.8095 - val_loss: 0.4387 - val_acc: 0.8013\n",
      "Epoch 19/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.4169 - acc: 0.8129 - val_loss: 0.4380 - val_acc: 0.8028\n",
      "Epoch 20/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4101 - acc: 0.8148 - val_loss: 0.4370 - val_acc: 0.8035\n",
      "Epoch 21/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4080 - acc: 0.8168 - val_loss: 0.4370 - val_acc: 0.8056\n",
      "Epoch 22/100\n",
      "1353/1353 [==============================] - 0s 51us/step - loss: 0.4037 - acc: 0.8190 - val_loss: 0.4367 - val_acc: 0.8062\n",
      "Epoch 23/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.3971 - acc: 0.8226 - val_loss: 0.4355 - val_acc: 0.8075\n",
      "Epoch 24/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.3972 - acc: 0.8213 - val_loss: 0.4350 - val_acc: 0.8081\n",
      "Epoch 25/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.3907 - acc: 0.8256 - val_loss: 0.4357 - val_acc: 0.8080\n",
      "Epoch 26/100\n",
      "1353/1353 [==============================] - 0s 54us/step - loss: 0.3875 - acc: 0.8258 - val_loss: 0.4356 - val_acc: 0.8080\n",
      "Epoch 27/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.3838 - acc: 0.8262 - val_loss: 0.4371 - val_acc: 0.8083\n",
      "Epoch 28/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.3773 - acc: 0.8278 - val_loss: 0.4358 - val_acc: 0.8090\n",
      "Epoch 29/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.3730 - acc: 0.8344 - val_loss: 0.4369 - val_acc: 0.8085\n",
      "Epoch 00029: early stopping\n",
      "Train on 1353 samples, validate on 581 samples\n",
      "Epoch 1/100\n",
      "1353/1353 [==============================] - 2s 1ms/step - loss: 0.6743 - acc: 0.6453 - val_loss: 0.6413 - val_acc: 0.7562\n",
      "Epoch 2/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.6012 - acc: 0.7601 - val_loss: 0.5403 - val_acc: 0.7665\n",
      "Epoch 3/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.5184 - acc: 0.7670 - val_loss: 0.5060 - val_acc: 0.7665\n",
      "Epoch 4/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.5076 - acc: 0.7645 - val_loss: 0.4916 - val_acc: 0.7687\n",
      "Epoch 5/100\n",
      "1353/1353 [==============================] - 0s 57us/step - loss: 0.4907 - acc: 0.7664 - val_loss: 0.4859 - val_acc: 0.7726\n",
      "Epoch 6/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4858 - acc: 0.7716 - val_loss: 0.4797 - val_acc: 0.7724\n",
      "Epoch 7/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.4771 - acc: 0.7771 - val_loss: 0.4735 - val_acc: 0.7782\n",
      "Epoch 8/100\n",
      "1353/1353 [==============================] - 0s 45us/step - loss: 0.4721 - acc: 0.7810 - val_loss: 0.4677 - val_acc: 0.7856\n",
      "Epoch 9/100\n",
      "1353/1353 [==============================] - 0s 45us/step - loss: 0.4625 - acc: 0.7881 - val_loss: 0.4622 - val_acc: 0.7863\n",
      "Epoch 10/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4562 - acc: 0.7908 - val_loss: 0.4570 - val_acc: 0.7910\n",
      "Epoch 11/100\n",
      "1353/1353 [==============================] - 0s 44us/step - loss: 0.4505 - acc: 0.7919 - val_loss: 0.4530 - val_acc: 0.7922\n",
      "Epoch 12/100\n",
      "1353/1353 [==============================] - 0s 46us/step - loss: 0.4423 - acc: 0.8003 - val_loss: 0.4499 - val_acc: 0.7937\n",
      "Epoch 13/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.4387 - acc: 0.7999 - val_loss: 0.4471 - val_acc: 0.7949\n",
      "Epoch 14/100\n",
      "1353/1353 [==============================] - 0s 45us/step - loss: 0.4361 - acc: 0.7998 - val_loss: 0.4449 - val_acc: 0.7981\n",
      "Epoch 15/100\n",
      "1353/1353 [==============================] - 0s 42us/step - loss: 0.4305 - acc: 0.8044 - val_loss: 0.4424 - val_acc: 0.7975\n",
      "Epoch 16/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4275 - acc: 0.8051 - val_loss: 0.4403 - val_acc: 0.7987\n",
      "Epoch 17/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.4223 - acc: 0.8064 - val_loss: 0.4398 - val_acc: 0.7969\n",
      "Epoch 18/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.4199 - acc: 0.8080 - val_loss: 0.4374 - val_acc: 0.7996\n",
      "Epoch 19/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.4138 - acc: 0.8124 - val_loss: 0.4358 - val_acc: 0.8005\n",
      "Epoch 20/100\n",
      "1353/1353 [==============================] - 0s 50us/step - loss: 0.4113 - acc: 0.8131 - val_loss: 0.4349 - val_acc: 0.8003\n",
      "Epoch 21/100\n",
      "1353/1353 [==============================] - 0s 51us/step - loss: 0.4074 - acc: 0.8141 - val_loss: 0.4340 - val_acc: 0.7999\n",
      "Epoch 22/100\n",
      "1353/1353 [==============================] - 0s 57us/step - loss: 0.4004 - acc: 0.8196 - val_loss: 0.4346 - val_acc: 0.8010\n",
      "Epoch 23/100\n",
      "1353/1353 [==============================] - 0s 53us/step - loss: 0.3962 - acc: 0.8190 - val_loss: 0.4341 - val_acc: 0.8017\n",
      "Epoch 24/100\n",
      "1353/1353 [==============================] - 0s 52us/step - loss: 0.3916 - acc: 0.8224 - val_loss: 0.4335 - val_acc: 0.8015\n",
      "Epoch 25/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.3916 - acc: 0.8235 - val_loss: 0.4343 - val_acc: 0.8007\n",
      "Epoch 26/100\n",
      "1353/1353 [==============================] - 0s 49us/step - loss: 0.3852 - acc: 0.8239 - val_loss: 0.4341 - val_acc: 0.8005\n",
      "Epoch 27/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.3811 - acc: 0.8275 - val_loss: 0.4349 - val_acc: 0.8006\n",
      "Epoch 28/100\n",
      "1353/1353 [==============================] - 0s 47us/step - loss: 0.3787 - acc: 0.8282 - val_loss: 0.4360 - val_acc: 0.8000\n",
      "Epoch 29/100\n",
      "1353/1353 [==============================] - 0s 48us/step - loss: 0.3737 - acc: 0.8303 - val_loss: 0.4361 - val_acc: 0.8015\n",
      "Epoch 00029: early stopping\n",
      "Micro-averaged f1 on cross validation: 0.6533531699578983\n"
     ]
    }
   ],
   "source": [
    "scores = []\n",
    "\n",
    "for train, test in cv.split(X, Y):\n",
    "    # Create and compile model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(200, activation=\"relu\", input_shape=(num_features,)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(200, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(200, activation=\"relu\"))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_labels, activation=\"sigmoid\"))\n",
    "\n",
    "    model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "    # Fit and make prediction\n",
    "    es = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=5, verbose=1)\n",
    "    model.fit(\n",
    "        X[train],\n",
    "        Y[train],\n",
    "        epochs=num_epochs,\n",
    "        batch_size=200,\n",
    "        verbose=1,\n",
    "        validation_split=0.3,\n",
    "        callbacks=[es]\n",
    "    )\n",
    "    Y_pred = (model.predict(X[test]) > 0.5).astype(np.uint8)\n",
    "\n",
    "    score = f1_score(Y[test], Y_pred, average=\"micro\")\n",
    "    scores.append(score)\n",
    "\n",
    "print(f\"Micro-averaged f1 on cross validation: {mean(scores)}\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "tensorflow-training",
   "language": "python",
   "name": "tensorflow-training"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
